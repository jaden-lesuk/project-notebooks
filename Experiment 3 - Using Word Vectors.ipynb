{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn import naive_bayes, svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnews_set = pd.read_csv('./kenyan-news/all-news.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 iterations\n",
    "w2v = gensim.models.Word2Vec(list(allnews_set['Total']), size=350, window=10, min_count=2, iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7fb0283f8640>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Word  Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordvectors(sentence):\n",
    "    sentence = [word for word in sentence if word in w2v.wv.vocab]\n",
    "    return np.mean(w2v[sentence], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "allnews_set['word_vectors'] = allnews_set['Total'].apply(generate_wordvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(allnews_set['word_vectors'], allnews_set['Tag'], test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1815    [0.0847119, 0.18145373, 0.0349021, 0.04074825,...\n",
       "704     [0.102782816, 0.18864474, 0.019293843, 0.05326...\n",
       "1136    [0.09616085, 0.20544122, 0.033075374, 0.033583...\n",
       "828     [0.09298691, 0.21591176, 0.038729552, 0.028182...\n",
       "159     [0.10379068, 0.22416198, 0.0380395, 0.03347276...\n",
       "                              ...                        \n",
       "835     [0.04337563, 0.16793858, 0.039883986, 0.073793...\n",
       "1216    [0.09344817, 0.18964249, 0.037496675, 0.040140...\n",
       "1653    [0.052056603, 0.17909586, 0.05970552, 0.015416...\n",
       "559     [0.086692594, 0.23544388, 0.03457702, 0.035829...\n",
       "684     [0.10867344, 0.20623219, 0.020832809, 0.040218...\n",
       "Name: word_vectors, Length: 1604, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = list(x_train)\n",
    "x_test = list(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.77      0.82      0.79       200\n",
      "         1.0       0.81      0.75      0.78       201\n",
      "\n",
      "    accuracy                           0.79       401\n",
      "   macro avg       0.79      0.79      0.79       401\n",
      "weighted avg       0.79      0.79      0.79       401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(C=1e5)\n",
    "clf.fit(x_train, y_train)\n",
    "predicted = clf.predict(x_test)\n",
    "print(classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.83      0.79       200\n",
      "         1.0       0.81      0.74      0.78       201\n",
      "\n",
      "    accuracy                           0.79       401\n",
      "   macro avg       0.79      0.79      0.79       401\n",
      "weighted avg       0.79      0.79      0.79       401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_mdl = svm.SVC(kernel = \"linear\", C = 1e2)\n",
    "svm_mdl.fit(x_train, y_train)\n",
    "predicted_svm = svm_mdl.predict(x_test)\n",
    "print(classification_report(y_test, predicted_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.80      0.72       200\n",
      "         1.0       0.75      0.60      0.67       201\n",
      "\n",
      "    accuracy                           0.70       401\n",
      "   macro avg       0.71      0.70      0.70       401\n",
      "weighted avg       0.71      0.70      0.70       401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "naive = naive_bayes.GaussianNB()\n",
    "naive.fit(x_train, y_train)\n",
    "predicted_naive = naive.predict(x_test)\n",
    "print(classification_report(y_test, predicted_naive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passsive Agressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.78      0.76       200\n",
      "         1.0       0.77      0.73      0.75       201\n",
      "\n",
      "    accuracy                           0.76       401\n",
      "   macro avg       0.76      0.76      0.76       401\n",
      "weighted avg       0.76      0.76      0.76       401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passagg = PassiveAggressiveClassifier(max_iter = 80)\n",
    "passagg.fit(x_train, y_train)\n",
    "predicted_pass = passagg.predict(x_test)\n",
    "print(classification_report(y_test, predicted_pass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
